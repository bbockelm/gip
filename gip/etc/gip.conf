[pbs]
pbs_path = /usr/pbs/bin
atlas_blacklist = *
cms_blacklist = *
cmsprod_blacklist = *
host = localhost
queue_exclude = pushpa,oli,aarond,osg
atlas_whitelist = atlas
default_blacklist = cms,dzero,atlas
workq_blacklist = cms,dzero,atlas
cmsprod_whitelist = cms
dzero_blacklist = *
dzero_whitelist = dzero
cms_whitelist = cms

[gip]
override = True

[site]
# We are working on making the GIP as flexible as possible, so you can have
# multiple clusters per site and multiple CEs per cluster.

# We are also working on keeping the SE independent of the CE.  Hence, you
# should be able to run the GIP standalone on your SE.  However, each CE
# *must* be able to tell what SEs it is associated with.  If you have a complex
# setup, then you will have to manually specify which SEs will be associated
# with your site.

# No matter what, an instance of the GIP must be run for every CE/SE.  Hence, we
# give you the option of not advertising the site for this copy of the GIP.
# Below, you will also have the option of

advertise_site = False
city = 
name = Nebraska
unique_name = red.unl.edu
country = 
longitude = 
latitude = 

[ce]
job_manager = pbs
name = red.unl.edu
unique_name = %(name)s
use_gt4 = True
gt4_endpoint = https://%(unique_name)s:9443
gt4_timeout = 15

[cluster]
advertise_cluster = True

# If simple is True, then we assume that the cluster has precisely one CE
simple = True
name = %(

# Have one section for each subcluster; call each section subcluster_X

# NOTE TO WLCG sites on the OSG: (ATLAS T2s, CMS T2s, etc):  If you don't fill
# in all your subclusters (or some reasonable approximation of them) or you
# don't update them, then your reported CPU contribution to the WLCG will most
# likely be normalized incorrectly.

# If you want all your contribution reported (it has been observed that some 
# sites which are only giving 25% of pledged amount have been underreporting
# by a factor of 2 or 3), FILL THESE IN CAREFULLY.

[subcluster_1]
name = Dell Opteron nodes
uniqueID=dellnodes
cores_per_node = 4
total_cores=160
total_cpus=80
ram_size = 8GB

# Do the worker nodes in this cluster have a public IP?
inbound_network = False

# Do they have outgoing access (as in, through a NAT)?
outbound_network = True

# If these are left as "UNDEFINED", then the information for the headnode will
# be used
os_name = UNDEFINED
os_release = UNDEFINED
os_version = UNDEFINED

cpu_speed_mhz=UNDEFINED
cpu_model=UNDEFINED
cpu_vendor=UNDEFINED

# An estimate of the per-core performance on the kSI00 and kSF00 benchmarks.
# These are now highly controversial; if you don't feel comfortable filling
# them in, leave them blank, and we will assign a number for you.

kSI00 = UNDEFINED
kSF00 = UNDEFINED

# If left undefined, the WN tmp directory will default to the OSG one.
wn_tmp=UNDEFINED
tmp=UNDEFINED

[cesebind]
# Traditional OSG setup.  One cluster per set of CEs, one storage element
# per CE
simple=True

# If simple is not true, then give a python dictionary of all the SEs which
# we should support.
se_list = ["T2_Nebraska_Storage", "REDDnet"]

[vo]
vo_blacklist = localUsers
vo_whitelist = dteam,ops

[osg_dirs]
app = /opt/osg/app
wn_tmp = /scratch
data = /opt/osg/data

[se]
advertise_se = True
name=T2_Nebraska_Storage
unique_name=srm.unl.edu

[bdii]
endpoint=ldap://is.grid.iu.edu:2170
#endpoint=ldap://is-itb2.grid.iu.edu:2170

[test]
goc=http://is.grid.iu.edu/cgi-bin/show_source_data?which=%s&source=cemon
#goc=http://is-itb2.grid.iu.edu/cgi-bin/show_source_data?which=%s&source=cemon
